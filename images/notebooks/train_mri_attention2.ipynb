{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reset random seeds\n",
    "def reset_random_seeds(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y.astype(int)  # Convert labels to integers\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32).permute(2, 0, 1), torch.tensor(self.y[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Layer\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute raw attention weights\n",
    "        attn_scores = self.conv1(x)\n",
    "        # Reshape to apply softmax over spatial dimensions\n",
    "        B, C, H, W = attn_scores.shape  # B: Batch, C: Channel, H: Height, W: Width\n",
    "        attn_scores_flat = attn_scores.view(B, C, -1)  # Flatten H and W\n",
    "        attn_weights_flat = F.softmax(attn_scores_flat, dim=2)  # Softmax over spatial dimensions\n",
    "        attn_weights = attn_weights_flat.view(B, C, H, W)  # Reshape back to original dimensions\n",
    "        return x * attn_weights, attn_weights\n",
    "\n",
    "\n",
    "# Modified MRI Model with Attention\n",
    "class AttentionMRIModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionMRIModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 100, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.conv2 = nn.Conv2d(100, 50, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.attention = AttentionLayer(50)\n",
    "        self.fc = nn.Linear(50 * 16 * 16, 3)  # Adjust size based on output from conv layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, attn_weights = self.attention(x)  # Apply attention\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "path = \"../data/processed/mri/\"\n",
    "\n",
    "def load_data():\n",
    "    with open(f\"{path}/img_train.pkl\", \"rb\") as fh:\n",
    "        data = pickle.load(fh)\n",
    "    X_train_ = pd.DataFrame(data)[\"img_array\"]\n",
    "\n",
    "    with open(f\"{path}/img_test.pkl\", \"rb\") as fh:\n",
    "        data = pickle.load(fh)\n",
    "    X_test_ = pd.DataFrame(data)[\"img_array\"]\n",
    "\n",
    "    with open(f\"{path}/img_y_train.pkl\", \"rb\") as fh:\n",
    "        data = pickle.load(fh)\n",
    "    y_train = np.array(pd.DataFrame(data)[\"label\"].values.astype(np.float32)).flatten()\n",
    "\n",
    "    with open(f\"{path}/img_y_test.pkl\", \"rb\") as fh:\n",
    "        data = pickle.load(fh)\n",
    "    y_test = np.array(pd.DataFrame(data)[\"label\"].values.astype(np.float32)).flatten()\n",
    "\n",
    "    y_train = np.where(y_train == 2, -1, y_train)\n",
    "    y_train = np.where(y_train == 1, 2, y_train)\n",
    "    y_train = np.where(y_train == -1, 1, y_train)\n",
    "\n",
    "    y_test = np.where(y_test == 2, -1, y_test)\n",
    "    y_test = np.where(y_test == 1, 2, y_test)\n",
    "    y_test = np.where(y_test == -1, 1, y_test)\n",
    "\n",
    "    X_train = np.array([X for X in X_train_.values])\n",
    "    X_test = np.array([X for X in X_test_.values])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(inputs, attn_weights, save_path):\n",
    "    \"\"\"\n",
    "    Visualizes the input MRI slices with corresponding attention maps.\n",
    "    \"\"\"\n",
    "    inputs = inputs.cpu().detach().numpy()  # Convert to numpy array\n",
    "    attn_weights = F.interpolate(\n",
    "        attn_weights, size=(72, 72), mode=\"bilinear\", align_corners=False\n",
    "    )  # Upsample attention weights to match input size\n",
    "    attn_weights = attn_weights.cpu().detach().numpy()  # Convert to numpy array\n",
    "\n",
    "    num_slices = inputs.shape[0]  # Number of slices in the batch\n",
    "    fig, axes = plt.subplots(num_slices, 3, figsize=(12, 4 * num_slices))\n",
    "    if num_slices == 1:  # Ensure axes are always iterable\n",
    "        axes = [axes]\n",
    "\n",
    "    for i in range(num_slices):\n",
    "        input_slice = inputs[i]  # Shape: (C, H, W)\n",
    "        if input_slice.shape[0] == 1:  # Grayscale input\n",
    "            input_slice = input_slice[0]  # Shape: (H, W)\n",
    "\n",
    "        # Original Input Slice\n",
    "        if len(input_slice.shape) == 2:  # Grayscale slice\n",
    "            axes[i][0].imshow(input_slice, cmap=\"gray\")\n",
    "        else:  # RGB slice\n",
    "            axes[i][0].imshow(input_slice.transpose(1, 2, 0))  # Convert CHW to HWC\n",
    "        axes[i][0].set_title(f\"Input Slice {i+1}\")\n",
    "\n",
    "        # Attention Map\n",
    "        axes[i][1].imshow(attn_weights[i, 0], cmap=\"hot\")  # Display single-channel attention\n",
    "        axes[i][1].set_title(f\"Attention Map Slice {i+1}\")\n",
    "\n",
    "        # Weighted Input\n",
    "        weighted_input = input_slice * attn_weights[i, 0]  # Apply attention weights\n",
    "        if len(weighted_input.shape) == 2:  # Grayscale weighted input\n",
    "            axes[i][2].imshow(weighted_input, cmap=\"hot\")\n",
    "        else:  # RGB weighted input\n",
    "            axes[i][2].imshow(weighted_input.transpose(1, 2, 0))  # Convert CHW to HWC\n",
    "        axes[i][2].set_title(f\"Weighted Input Slice {i+1}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation Functions\n",
    "def train_model(model, train_loader, device, epochs=20):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        if epoch % 10 == 9:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    path = \"../outputs/mri\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_attn_weights = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs, attn_weights = model(data)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_attn_weights.append(attn_weights.cpu().numpy())\n",
    "\n",
    "    # Convert collected lists to arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_attn_weights = np.concatenate(all_attn_weights, axis=0)  # Shape: (N, C, H, W)\n",
    "\n",
    "    # 1. Classification Report\n",
    "    report = classification_report(all_targets, all_preds, target_names=class_names)\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    # 2. Confusion Matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(f\"{path}/mri_attn2_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Attention Weight Visualization\n",
    "    # Visualize attention weights for a few test samples\n",
    "    num_samples_to_visualize = min(5, len(test_loader.dataset))\n",
    "\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        if i >= num_samples_to_visualize:\n",
    "            break\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        _, attn_weights = model(data)\n",
    "        save_path = f\"../outputs/mri/attention_weights_sample_{i+1}.png\"\n",
    "        visualize_attention(data[0], attn_weights, save_path)\n",
    "\n",
    "    # 4. Slice Importance\n",
    "    # Average attention weights across height and width to determine slice importance\n",
    "    slice_importances = all_attn_weights.mean(axis=(2, 3))  # Shape: (N, C)\n",
    "    average_importance = slice_importances.mean(axis=0)  # Shape: (C,)\n",
    "\n",
    "    print(slice_importances.shape)\n",
    "    print(average_importance.shape)\n",
    "\n",
    "    # Print slice importances\n",
    "    for i, importance in enumerate(average_importance, start=1):\n",
    "        print(f\"Slice {i} Importance: {importance:.4f}\")\n",
    "\n",
    "    # Plot slice importance\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(range(1, 4), average_importance, tick_label=[\"Slice 1\", \"Slice 2\", \"Slice 3\"], color=\"skyblue\")\n",
    "    plt.title(\"Slice Importance\")\n",
    "    plt.xlabel(\"Slice\")\n",
    "    plt.ylabel(\"Average Attention Weight\")\n",
    "    plt.savefig(f\"{path}/mri_attn2_slice_importance.png\")\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Evaluation complete. Results saved:\")\n",
    "    print(\"- Classification report printed in console.\")\n",
    "    print(\"- Confusion matrix saved as confusion_matrix.png.\")\n",
    "    print(\"- Attention weights visualization saved as attention_weights.png.\")\n",
    "    print(\"- Slice importance plot saved as slice_importance.png.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main driver code\n",
    "X_train, X_test, y_train, y_test = load_data()\n",
    "\n",
    "train_dataset = MRIDataset(X_train, y_train)\n",
    "test_dataset = MRIDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_names = [\"CN\", \"MCI\", \"AD\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/250, Loss: 0.8890260457992554\n",
      "Epoch 20/250, Loss: 0.6775628924369812\n",
      "Epoch 30/250, Loss: 0.7630151510238647\n",
      "Epoch 40/250, Loss: 0.6653193235397339\n",
      "Epoch 50/250, Loss: 0.22531187534332275\n",
      "Epoch 60/250, Loss: 0.4359924793243408\n",
      "Epoch 70/250, Loss: 0.2643098831176758\n",
      "Epoch 80/250, Loss: 0.47648367285728455\n",
      "Epoch 90/250, Loss: 0.2895069122314453\n",
      "Epoch 100/250, Loss: 0.2881888449192047\n",
      "Epoch 110/250, Loss: 0.32141241431236267\n",
      "Epoch 120/250, Loss: 0.3178722858428955\n",
      "Epoch 130/250, Loss: 0.06007321923971176\n",
      "Epoch 140/250, Loss: 0.06855204701423645\n",
      "Epoch 150/250, Loss: 0.4204598069190979\n",
      "Epoch 160/250, Loss: 0.19609060883522034\n",
      "Epoch 170/250, Loss: 0.20010161399841309\n",
      "Epoch 180/250, Loss: 0.10602924972772598\n",
      "Epoch 190/250, Loss: 0.016724903136491776\n",
      "Epoch 200/250, Loss: 0.07689861953258514\n",
      "Epoch 210/250, Loss: 0.20947209000587463\n",
      "Epoch 220/250, Loss: 0.06550080329179764\n",
      "Epoch 230/250, Loss: 0.09307173639535904\n",
      "Epoch 240/250, Loss: 0.02349722571671009\n",
      "Epoch 250/250, Loss: 0.05928821489214897\n"
     ]
    }
   ],
   "source": [
    "model = AttentionMRIModel()\n",
    "train_model(model, train_loader, device, epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          CN       0.96      1.00      0.98        26\n",
      "         MCI       1.00      1.00      1.00         6\n",
      "          AD       1.00      0.83      0.91         6\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.99      0.94      0.96        38\n",
      "weighted avg       0.97      0.97      0.97        38\n",
      "\n",
      "(38, 1)\n",
      "(1,)\n",
      "Slice 1 Importance: 0.0039\n",
      "Evaluation complete. Results saved:\n",
      "- Classification report printed in console.\n",
      "- Confusion matrix saved as confusion_matrix.png.\n",
      "- Attention weights visualization saved as attention_weights.png.\n",
      "- Slice importance plot saved as slice_importance.png.\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
